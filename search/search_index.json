{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"OpenWebUI Client Library","text":"<p>A Python client library for OpenWebUI, providing a seamless interface to interact with OpenWebUI's API.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Compatible interface with the OpenAI Python SDK</li> <li>Support for chat completions with file attachments</li> <li>Access to available models in your OpenWebUI instance</li> <li>File upload and management capabilities</li> <li>Environment variable configuration</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#030-2025-07-17","title":"[0.3.0] - 2025-07-17","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Added models functionality</li> <li>Added comprehensive documentation for the models functionality</li> </ul>"},{"location":"changelog/#021-2025-07-05","title":"[0.2.1] - 2025-07-05","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Fixed type error in <code>files.py</code> related to multipart form data</li> <li>Updated type annotations to use <code>Union[]</code> instead of pipe operator (<code>|</code>) for better compatibility</li> <li>Increased HTTP request timeout from 45 to 60 seconds</li> </ul>"},{"location":"changelog/#020-2025-06-15","title":"[0.2.0] - 2025-06-15","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Support for function calling and tools</li> <li>Tool registry for automatic function registration</li> <li>Extended completions class with file support</li> </ul>"},{"location":"changelog/#010-2025-05-27","title":"[0.1.0] - 2025-05-27","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Initial release</li> <li>Basic client functionality</li> <li>Support for file uploads</li> <li>Support for file attachments in chat completions</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to the OpenWebUI client library! This document provides guidelines and instructions for contributing.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/bemade/openwebui-client.git\ncd openwebui-client\n</code></pre> <ol> <li>Create and activate a virtual environment:</li> </ol> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n</code></pre> <ol> <li>Install the package in development mode:</li> </ol> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use the following tools to maintain code quality:</p> <ul> <li>Black: For code formatting</li> <li>isort: For import sorting</li> <li>mypy: For static type checking</li> <li>flake8: For code style enforcement</li> </ul> <p>You can run these tools using pre-commit:</p> <pre><code>pre-commit install  # Install the git hooks\npre-commit run --all-files  # Run on all files\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<p>Run the tests with pytest:</p> <pre><code>pytest\n</code></pre> <p>For tests with coverage:</p> <pre><code>pytest --cov=openwebui_client\n</code></pre>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<p>To build the documentation:</p> <pre><code>mkdocs build\n</code></pre> <p>The documentation will be available in the <code>site</code> directory.</p>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":"<ol> <li>Create a branch for your changes:</li> </ol> <pre><code>git checkout -b feature-branch-name\n</code></pre> <ol> <li>Make your changes and commit them:</li> </ol> <pre><code>git commit -m \"Description of changes\"\n</code></pre> <ol> <li>Push your changes to your fork:</li> </ol> <pre><code>git push origin feature-branch-name\n</code></pre> <ol> <li>Open a pull request against the main repository.</li> </ol>"},{"location":"examples/","title":"Examples","text":""},{"location":"examples/#basic-chat-completion","title":"Basic Chat Completion","text":"<pre><code>from openwebui_client import OpenWebUIClient\n\nclient = OpenWebUIClient(\n    api_key=\"your-openwebui-api-key\",\n    base_url=\"http://localhost:5000\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What can you tell me about OpenWebUI?\"}\n    ]\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/#chat-completion-with-files","title":"Chat Completion with Files","text":"<pre><code>from openwebui_client import OpenWebUIClient\n\nclient = OpenWebUIClient(\n    api_key=\"your-openwebui-api-key\",\n    base_url=\"http://localhost:5000\"\n)\n\n# Read file content\nwith open(\"document.pdf\", \"rb\") as f:\n    file_content = f.read()\n\n# Create chat completion with file attachment\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Summarize the attached document.\"}\n    ],\n    files=[file_content]\n)\n</code></pre>"},{"location":"examples/#listing-available-models","title":"Listing Available Models","text":"<pre><code>from openwebui_client import OpenWebUIClient\n\nclient = OpenWebUIClient(\n    api_key=\"your-openwebui-api-key\",\n    base_url=\"http://localhost:5000\"\n)\n\n# List all available models\nmodels = client.models.list()\n\n# Print model information\nprint(f\"Found {len(models)} models:\")\nfor model in models:\n    print(f\"ID: {model.id}, Name: {model.name or model.id}, Owner: {model.owned_by}\")\n\n# Find models with specific characteristics\nlocal_models = [m for m in models if \"local\" in m.id.lower() or (m.name and \"local\" in m.name.lower())]\nprint(f\"\\nFound {len(local_models)} local models:\")\nfor model in local_models:\n    print(f\"ID: {model.id}, Name: {model.name or model.id}\")\n</code></pre>"},{"location":"examples/#using-function-calling-tools","title":"Using Function Calling / Tools","text":""},{"location":"examples/#direct-tool-usage","title":"Direct Tool Usage","text":"<pre><code>from openwebui_client import OpenWebUIClient\n\nclient = OpenWebUIClient(\n    api_key=\"your-openwebui-api-key\",\n    base_url=\"http://localhost:5000\"\n)\n\n# Define a tool schema directly\nresponse = client.chat.completions.create(\n    model=\"your_model\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is the current time?\"}\n    ],\n    tools=[\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"get_current_time\",\n                \"description\": \"Get the current time.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {},\n                    \"required\": [],\n                },\n            },\n        }\n    ],\n)\n\n# Check if the model used tools\nif response.choices[0].message.tool_calls:\n    tool_call = response.choices[0].message.tool_calls[0]\n    print(f\"Tool called: {tool_call.function.name}\")\n</code></pre>"},{"location":"examples/#using-the-tool-registry","title":"Using the Tool Registry","text":"<pre><code>from openwebui_client import OpenWebUIClient\nfrom pathlib import Path\n\nclient = OpenWebUIClient(\n    api_key=\"your-openwebui-api-key\",\n    base_url=\"http://localhost:5000\"\n)\n\n# Define tool functions\ndef get_weather(location: str, unit: str = \"celsius\") -&gt; str:\n    \"\"\"Get the current weather in a given location.\n\n    Args:\n        location: The location to get weather for\n        unit: The temperature unit to use (celsius or fahrenheit)\n\n    Returns:\n        str: A string describing the current weather\n    \"\"\"\n    return f\"The weather in {location} is sunny and 25\u00b0{unit[0]}\"\n\ndef get_forecast(location: str, days: int = 1) -&gt; str:\n    \"\"\"Get the weather forecast for a location.\n\n    Args:\n        location: The location to get the forecast for\n        days: Number of days to forecast\n\n    Returns:\n        str: A string describing the forecast\n    \"\"\"\n    return f\"The forecast for {location} for the next {days} day(s) is sunny.\"\n\n# Register tools with the client\nclient.tool_registry.register(get_weather)\nclient.tool_registry.register(get_forecast)\n\n# Use chat_with_tools for automatic tool handling\nresponse = client.chat_with_tools(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n        {\"role\": \"user\", \"content\": \"What's the weather like in Toronto?\"}\n    ],\n    max_tool_calls=5,\n)\n\nprint(response)  # Will contain the final response after any tool calls\n\n# You can also include files with tool calls\nfile_path = Path(\"weather_data.txt\")\nresponse_with_file = client.chat_with_tools(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n        {\"role\": \"user\", \"content\": \"Analyze the weather data in the attached file.\"}\n    ],\n    files=[file_path],\n    max_tool_calls=5,\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"examples/#file-upload-and-management","title":"File Upload and Management","text":"<pre><code>from openwebui_client import OpenWebUIClient\n\nclient = OpenWebUIClient(\n    api_key=\"your-openwebui-api-key\",\n    base_url=\"http://localhost:5000\"\n)\n\n# Read file content\nwith open(\"document.pdf\", \"rb\") as f:\n    file_content = f.read()\n\n# Upload a single file\nfile_obj = client.files.create(\n    file=file_content,\n    file_metadata={\"purpose\": \"assistants\"}\n)\nprint(f\"File uploaded with ID: {file_obj.id}\")\n\n# Upload multiple files\nwith open(\"another_doc.pdf\", \"rb\") as f2:\n    file_content2 = f2.read()\n\nfile_objects = client.files.create(\n    files=[(file_content, {\"purpose\": \"assistants\"}),\n           (file_content2, {\"purpose\": \"assistants\"})]\n)\nfor i, file_obj in enumerate(file_objects):\n    print(f\"File {i+1} uploaded with ID: {file_obj.id}\")\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>OpenAI Python SDK</li> </ul>"},{"location":"installation/#installation-from-pypi","title":"Installation from PyPI","text":"<p>The recommended way to install the OpenWebUI client is via pip:</p> <pre><code>pip install openwebui-client\n</code></pre>"},{"location":"installation/#installation-from-source","title":"Installation from Source","text":"<p>If you prefer to install from source:</p> <pre><code>git clone https://github.com/mdurepos/openwebui-client.git\ncd openwebui-client\npip install -e .\n</code></pre>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For development, you'll want to install the package with development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This will install additional packages required for development, such as pytest, black, isort, and mypy.</p>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#configuration","title":"Configuration","text":"<p>The OpenWebUI client can be configured using environment variables or by passing parameters directly to the client constructor.</p>"},{"location":"usage/#environment-variables","title":"Environment Variables","text":"<ul> <li><code>OPENWEBUI_API_KEY</code>: Your OpenWebUI API key</li> <li><code>OPENWEBUI_API_BASE</code>: Base URL for your OpenWebUI instance (default: http://localhost:5000)</li> </ul>"},{"location":"usage/#direct-configuration","title":"Direct Configuration","text":"<pre><code>from openwebui_client import OpenWebUIClient\n\nclient = OpenWebUIClient(\n    api_key=\"your-openwebui-api-key\",\n    base_url=\"http://localhost:5000\",\n    default_model=\"gpt-4\"\n)\n</code></pre>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":""},{"location":"usage/#chat-completions","title":"Chat Completions","text":"<p>Create a simple chat completion:</p> <pre><code>response = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is OpenWebUI?\"}\n    ]\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"usage/#chat-completions-with-files","title":"Chat Completions with Files","text":"<p>Include files with your chat completions:</p> <pre><code>with open(\"document.pdf\", \"rb\") as f:\n    file_content = f.read()\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Summarize the attached document.\"}\n    ],\n    files=[file_content]\n)\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"usage/#models","title":"Models","text":""},{"location":"usage/#list-available-models","title":"List Available Models","text":"<p>Retrieve a list of all available models in your OpenWebUI instance:</p> <pre><code># Get all available models\nmodels = client.models.list()\n\n# Print model information\nfor model in models:\n    print(f\"ID: {model.id}, Name: {model.name or model.id}\")\n</code></pre>"},{"location":"usage/#access-model-properties","title":"Access Model Properties","text":"<p>Each model has several properties you can access:</p> <pre><code># Get the first model\nmodel = client.models.list()[0]\n\n# Access model properties\nprint(f\"Model ID: {model.id}\")\nprint(f\"Model Name: {model.name}\")\nprint(f\"Created: {model.created}\")\nprint(f\"Owner: {model.owned_by}\")\n</code></pre> <p>See the Models API documentation for more details.</p>"},{"location":"usage/#file-management","title":"File Management","text":""},{"location":"usage/#upload-a-single-file","title":"Upload a Single File","text":"<pre><code>with open(\"document.pdf\", \"rb\") as f:\n    file_content = f.read()\n\nfile_obj = client.files.create(\n    file=file_content,\n    file_metadata={\"purpose\": \"assistants\"}\n)\nprint(f\"File uploaded with ID: {file_obj.id}\")\n</code></pre>"},{"location":"usage/#upload-multiple-files","title":"Upload Multiple Files","text":"<pre><code>file_objects = client.files.create(\n    files=[\n        (file_content1, {\"purpose\": \"assistants\"}),\n        (file_content2, {\"purpose\": \"assistants\"})\n    ]\n)\nprint(f\"Uploaded {len(file_objects)} files\")\n</code></pre>"},{"location":"api/client/","title":"Client","text":""},{"location":"api/client/#openwebui_client.client","title":"<code>openwebui_client.client</code>","text":"<p>OpenWebUI client for interacting with the OpenWebUI API.</p>"},{"location":"api/client/#openwebui_client.client.OpenWebUIChat","title":"<code>OpenWebUIChat</code>","text":"<p>               Bases: <code>Chat</code></p> <p>Custom Chat class that uses OpenWebUICompletions.</p> Source code in <code>openwebui_client/client.py</code> <pre><code>class OpenWebUIChat(OpenAIChat):\n    \"\"\"Custom Chat class that uses OpenWebUICompletions.\"\"\"\n\n    @cached_property\n    def completions(self) -&gt; OpenWebUICompletions:\n        return OpenWebUICompletions(self._client)\n</code></pre>"},{"location":"api/client/#openwebui_client.client.OpenWebUIClient","title":"<code>OpenWebUIClient</code>","text":"<p>               Bases: <code>OpenAI</code></p> <p>Client for interacting with the OpenWebUI API.</p> <p>This client extends the OpenAI client with OpenWebUI-specific features like file attachments in chat completions.</p> Source code in <code>openwebui_client/client.py</code> <pre><code>class OpenWebUIClient(OpenAI):\n    \"\"\"Client for interacting with the OpenWebUI API.\n\n    This client extends the OpenAI client with OpenWebUI-specific\n    features like file attachments in chat completions.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: Optional[str] = None,\n        base_url: str = \"http://localhost:5000\",\n        default_model: str = \"gpt-4\",\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"Initialize the OpenWebUI client.\n\n        Args:\n            api_key: Your OpenWebUI API key\n            base_url: Base URL for the API (defaults to OpenWebUI's local instance)\n            default_model: Default model to use for completions\n            **kwargs: Additional arguments to pass to the OpenAI client\n        \"\"\"\n        # OpenWebUI has different endpoint patterns than OpenAI\n        # Remove trailing slash if present\n        if base_url.endswith(\"/\"):\n            base_url = base_url[:-1]\n\n        # Initialize the parent OpenAI class\n        super().__init__(api_key=api_key, base_url=base_url, **kwargs)\n\n        # Store additional configuration\n        self.default_model = default_model\n        self.base_url = base_url\n        self.tool_registry = ToolsRegistry()\n\n    @cached_property\n    def chat(self) -&gt; OpenWebUIChat:\n        \"\"\"Return the custom OpenWebUIChat instance.\"\"\"\n        return OpenWebUIChat(self)\n\n    @cached_property\n    def files(self) -&gt; OpenWebUIFiles:\n        return OpenWebUIFiles(self)\n\n    @cached_property\n    def models(self) -&gt; OpenWebUIModels:\n        \"\"\"Return the custom OpenWebUIModels instance.\"\"\"\n        return OpenWebUIModels(self)\n\n    def chat_with_tools(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        tools: Optional[Sequence[str]] = None,\n        tool_params: Optional[Dict[str, Dict[str, Any]]] = None,\n        model: Optional[str] = None,\n        max_tool_calls: int = 5,\n        files: Iterable[Path] = [],\n    ) -&gt; str:\n        \"\"\"Send a chat completion request and handle tool calls automatically.\n\n        This will automatically execute tool calls and include their results\n        in subsequent API calls until the model returns a final response.\n\n        Args:\n            messages: List of message dictionaries with 'role' and 'content' keys\n            tools: List of tool names to use (None for all registered tools)\n            tool_params: Optional parameters to pass to the tools when they are called\n            model: Model to use (defaults to the client's default model)\n            max_tool_calls: Maximum number of tool call rounds to allow\n            files: Optional list of Path objects to files that should be included with the request\n\n        Returns:\n            The final assistant message content (str) after all tool calls are processed\n\n        Raises:\n            RuntimeError: If the maximum number of tool calls is exceeded\n\n        Example:\n            &gt;&gt;&gt; client = OpenWebUIClient()\n            &gt;&gt;&gt; client.tool_registry.register(get_weather)\n            &gt;&gt;&gt; response = client.chat_with_tools(\n            ...     messages=[{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}],\n            ...     max_tool_calls=3\n            ... )\n            &gt;&gt;&gt; print(response)\n        \"\"\"\n        # Initialize tool_params if not provided\n        if tool_params is None:\n            tool_params = {}\n\n        conversation: List[ChatCompletionMessageParam] = messages.copy()\n        file_refs = self.files.from_paths([(file, None) for file in files])\n\n        # Conversation is now a list that we can mutate\n        tool_call_count = 0\n\n        # Get tools from the registry\n        all_tools = self.tool_registry.get_openai_tools()\n\n        # Filter tools if specific ones were requested\n        if tools:\n            tool_schemas = [\n                tool\n                for tool in all_tools\n                if tool.get(\"function\", {}).get(\"name\") in tools\n            ]\n        else:\n            # Use all registered tools\n            tool_schemas = all_tools\n\n        _logger.debug(\"Starting chat with tools\")\n        _logger.debug(\n            f\"Available tools: {[t['function']['name'] for t in tool_schemas] if tool_schemas else 'None'}\"\n        )\n        _logger.debug(f\"Initial messages: {conversation}\")\n\n        while tool_call_count &lt; max_tool_calls:\n            # Get the next response from the model\n            _logger.debug(\n                f\"Sending request to model (attempt {tool_call_count + 1}/{max_tool_calls})\"\n            )\n            _logger.debug(f\"Messages: {conversation}\")\n            _logger.debug(f\"Using model: {model or self.default_model}\")\n            _logger.debug(\n                f\"Tools: {json.dumps(tool_schemas, indent=4)} if tool_schemas else 'None'\"\n            )\n\n            # Debug log the tool dictionaries\n            _logger.debug(f\"Tool dictionaries: {json.dumps(tool_schemas, indent=2)}\")\n            # Also log the original tool schemas for comparison\n\n            args = {\n                \"messages\": conversation,\n                \"model\": model or self.default_model,\n                \"tools\": tool_schemas,\n                \"tool_choice\": \"auto\",\n                \"files\": file_refs,\n            }\n            _logger.debug(f\"Args: {args}\")\n            response = self.chat.completions.create(**args)\n\n            _logger.debug(f\"Received response: {response}\")\n\n            # Not running in stream mode, this should never fail. Here for type safety.\n            assert isinstance(response, ChatCompletion)\n            message = response.choices[0].message\n\n            # If there are no tool calls, we're done\n            if not hasattr(message, \"tool_calls\") or not message.tool_calls:\n                _logger.debug(\"No tool calls in response, ending conversation\")\n                return message.content or \"\"\n            response = self.chat.completions.create(**args)\n\n            _logger.debug(f\"Received response: {response}\")\n\n            # Not running in stream mode, this should never fail. Here for type safety.\n            assert isinstance(response, ChatCompletion)\n            message = response.choices[0].message\n\n            # If there are no tool calls, we're done\n            if not hasattr(message, \"tool_calls\") or not message.tool_calls:\n                _logger.debug(\"No tool calls in response, ending conversation\")\n                return message.content or \"\"\n\n            # Process tool calls\n            tool_call_count += 1\n            _logger.debug(f\"Processing tool call {tool_call_count}/{max_tool_calls}\")\n\n            for tool_call in message.tool_calls:\n                function = tool_call.function\n                non_ai_params = tool_params.get(function.name, {})\n                _logger.debug(f\"Calling tool: {function.name}\")\n                _logger.debug(f\"Arguments: {function.arguments}\")\n                if non_ai_params:\n                    _logger.debug(f\"Non-AI parameters: {non_ai_params}\")\n\n                # Execute the tool\n                try:\n                    _logger.debug(\n                        f\"Calling tool: {function.name} with args: {function.arguments}\"\n                    )\n                    result = self.tool_registry.call_tool(\n                        function.name,\n                        json.loads(function.arguments),\n                        non_ai_params=non_ai_params,\n                    )\n                    result_str = (\n                        json.dumps(result) if not isinstance(result, str) else result\n                    )\n                    _logger.debug(\n                        f\"Tool {function.name} returned: {result_str[:200]}...\"\n                        if len(str(result_str)) &gt; 200\n                        else f\"Tool {function.name} returned: {result_str}\"\n                    )\n                except Exception as e:\n                    result_str = f\"Error: {e!s}\"\n                    _logger.error(\n                        f\"Error calling tool {function.name}: {e}\", exc_info=True\n                    )\n\n                # Add the tool response to the conversation\n                conversation.append(\n                    ChatCompletionToolMessageParam(\n                        tool_call_id=tool_call.id,\n                        role=\"tool\",\n                        content=result_str,\n                    )\n                )\n\n        raise RuntimeError(f\"Maximum number of tool calls ({max_tool_calls}) exceeded\")\n</code></pre>"},{"location":"api/client/#openwebui_client.client.OpenWebUIClient.__init__","title":"<code>__init__(api_key=None, base_url='http://localhost:5000', default_model='gpt-4', **kwargs)</code>","text":"<p>Initialize the OpenWebUI client.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>Optional[str]</code> <p>Your OpenWebUI API key</p> <code>None</code> <code>base_url</code> <code>str</code> <p>Base URL for the API (defaults to OpenWebUI's local instance)</p> <code>'http://localhost:5000'</code> <code>default_model</code> <code>str</code> <p>Default model to use for completions</p> <code>'gpt-4'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments to pass to the OpenAI client</p> <code>{}</code> Source code in <code>openwebui_client/client.py</code> <pre><code>def __init__(\n    self,\n    api_key: Optional[str] = None,\n    base_url: str = \"http://localhost:5000\",\n    default_model: str = \"gpt-4\",\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Initialize the OpenWebUI client.\n\n    Args:\n        api_key: Your OpenWebUI API key\n        base_url: Base URL for the API (defaults to OpenWebUI's local instance)\n        default_model: Default model to use for completions\n        **kwargs: Additional arguments to pass to the OpenAI client\n    \"\"\"\n    # OpenWebUI has different endpoint patterns than OpenAI\n    # Remove trailing slash if present\n    if base_url.endswith(\"/\"):\n        base_url = base_url[:-1]\n\n    # Initialize the parent OpenAI class\n    super().__init__(api_key=api_key, base_url=base_url, **kwargs)\n\n    # Store additional configuration\n    self.default_model = default_model\n    self.base_url = base_url\n    self.tool_registry = ToolsRegistry()\n</code></pre>"},{"location":"api/client/#openwebui_client.client.OpenWebUIClient.chat","title":"<code>chat()</code>","text":"<p>Return the custom OpenWebUIChat instance.</p> Source code in <code>openwebui_client/client.py</code> <pre><code>@cached_property\ndef chat(self) -&gt; OpenWebUIChat:\n    \"\"\"Return the custom OpenWebUIChat instance.\"\"\"\n    return OpenWebUIChat(self)\n</code></pre>"},{"location":"api/client/#openwebui_client.client.OpenWebUIClient.chat_with_tools","title":"<code>chat_with_tools(messages, tools=None, tool_params=None, model=None, max_tool_calls=5, files=[])</code>","text":"<p>Send a chat completion request and handle tool calls automatically.</p> <p>This will automatically execute tool calls and include their results in subsequent API calls until the model returns a final response.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[ChatCompletionMessageParam]</code> <p>List of message dictionaries with 'role' and 'content' keys</p> required <code>tools</code> <code>Optional[Sequence[str]]</code> <p>List of tool names to use (None for all registered tools)</p> <code>None</code> <code>tool_params</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Optional parameters to pass to the tools when they are called</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Model to use (defaults to the client's default model)</p> <code>None</code> <code>max_tool_calls</code> <code>int</code> <p>Maximum number of tool call rounds to allow</p> <code>5</code> <code>files</code> <code>Iterable[Path]</code> <p>Optional list of Path objects to files that should be included with the request</p> <code>[]</code> <p>Returns:</p> Type Description <code>str</code> <p>The final assistant message content (str) after all tool calls are processed</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the maximum number of tool calls is exceeded</p> Example <p>client = OpenWebUIClient() client.tool_registry.register(get_weather) response = client.chat_with_tools( ...     messages=[{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}], ...     max_tool_calls=3 ... ) print(response)</p> Source code in <code>openwebui_client/client.py</code> <pre><code>def chat_with_tools(\n    self,\n    messages: List[ChatCompletionMessageParam],\n    tools: Optional[Sequence[str]] = None,\n    tool_params: Optional[Dict[str, Dict[str, Any]]] = None,\n    model: Optional[str] = None,\n    max_tool_calls: int = 5,\n    files: Iterable[Path] = [],\n) -&gt; str:\n    \"\"\"Send a chat completion request and handle tool calls automatically.\n\n    This will automatically execute tool calls and include their results\n    in subsequent API calls until the model returns a final response.\n\n    Args:\n        messages: List of message dictionaries with 'role' and 'content' keys\n        tools: List of tool names to use (None for all registered tools)\n        tool_params: Optional parameters to pass to the tools when they are called\n        model: Model to use (defaults to the client's default model)\n        max_tool_calls: Maximum number of tool call rounds to allow\n        files: Optional list of Path objects to files that should be included with the request\n\n    Returns:\n        The final assistant message content (str) after all tool calls are processed\n\n    Raises:\n        RuntimeError: If the maximum number of tool calls is exceeded\n\n    Example:\n        &gt;&gt;&gt; client = OpenWebUIClient()\n        &gt;&gt;&gt; client.tool_registry.register(get_weather)\n        &gt;&gt;&gt; response = client.chat_with_tools(\n        ...     messages=[{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}],\n        ...     max_tool_calls=3\n        ... )\n        &gt;&gt;&gt; print(response)\n    \"\"\"\n    # Initialize tool_params if not provided\n    if tool_params is None:\n        tool_params = {}\n\n    conversation: List[ChatCompletionMessageParam] = messages.copy()\n    file_refs = self.files.from_paths([(file, None) for file in files])\n\n    # Conversation is now a list that we can mutate\n    tool_call_count = 0\n\n    # Get tools from the registry\n    all_tools = self.tool_registry.get_openai_tools()\n\n    # Filter tools if specific ones were requested\n    if tools:\n        tool_schemas = [\n            tool\n            for tool in all_tools\n            if tool.get(\"function\", {}).get(\"name\") in tools\n        ]\n    else:\n        # Use all registered tools\n        tool_schemas = all_tools\n\n    _logger.debug(\"Starting chat with tools\")\n    _logger.debug(\n        f\"Available tools: {[t['function']['name'] for t in tool_schemas] if tool_schemas else 'None'}\"\n    )\n    _logger.debug(f\"Initial messages: {conversation}\")\n\n    while tool_call_count &lt; max_tool_calls:\n        # Get the next response from the model\n        _logger.debug(\n            f\"Sending request to model (attempt {tool_call_count + 1}/{max_tool_calls})\"\n        )\n        _logger.debug(f\"Messages: {conversation}\")\n        _logger.debug(f\"Using model: {model or self.default_model}\")\n        _logger.debug(\n            f\"Tools: {json.dumps(tool_schemas, indent=4)} if tool_schemas else 'None'\"\n        )\n\n        # Debug log the tool dictionaries\n        _logger.debug(f\"Tool dictionaries: {json.dumps(tool_schemas, indent=2)}\")\n        # Also log the original tool schemas for comparison\n\n        args = {\n            \"messages\": conversation,\n            \"model\": model or self.default_model,\n            \"tools\": tool_schemas,\n            \"tool_choice\": \"auto\",\n            \"files\": file_refs,\n        }\n        _logger.debug(f\"Args: {args}\")\n        response = self.chat.completions.create(**args)\n\n        _logger.debug(f\"Received response: {response}\")\n\n        # Not running in stream mode, this should never fail. Here for type safety.\n        assert isinstance(response, ChatCompletion)\n        message = response.choices[0].message\n\n        # If there are no tool calls, we're done\n        if not hasattr(message, \"tool_calls\") or not message.tool_calls:\n            _logger.debug(\"No tool calls in response, ending conversation\")\n            return message.content or \"\"\n        response = self.chat.completions.create(**args)\n\n        _logger.debug(f\"Received response: {response}\")\n\n        # Not running in stream mode, this should never fail. Here for type safety.\n        assert isinstance(response, ChatCompletion)\n        message = response.choices[0].message\n\n        # If there are no tool calls, we're done\n        if not hasattr(message, \"tool_calls\") or not message.tool_calls:\n            _logger.debug(\"No tool calls in response, ending conversation\")\n            return message.content or \"\"\n\n        # Process tool calls\n        tool_call_count += 1\n        _logger.debug(f\"Processing tool call {tool_call_count}/{max_tool_calls}\")\n\n        for tool_call in message.tool_calls:\n            function = tool_call.function\n            non_ai_params = tool_params.get(function.name, {})\n            _logger.debug(f\"Calling tool: {function.name}\")\n            _logger.debug(f\"Arguments: {function.arguments}\")\n            if non_ai_params:\n                _logger.debug(f\"Non-AI parameters: {non_ai_params}\")\n\n            # Execute the tool\n            try:\n                _logger.debug(\n                    f\"Calling tool: {function.name} with args: {function.arguments}\"\n                )\n                result = self.tool_registry.call_tool(\n                    function.name,\n                    json.loads(function.arguments),\n                    non_ai_params=non_ai_params,\n                )\n                result_str = (\n                    json.dumps(result) if not isinstance(result, str) else result\n                )\n                _logger.debug(\n                    f\"Tool {function.name} returned: {result_str[:200]}...\"\n                    if len(str(result_str)) &gt; 200\n                    else f\"Tool {function.name} returned: {result_str}\"\n                )\n            except Exception as e:\n                result_str = f\"Error: {e!s}\"\n                _logger.error(\n                    f\"Error calling tool {function.name}: {e}\", exc_info=True\n                )\n\n            # Add the tool response to the conversation\n            conversation.append(\n                ChatCompletionToolMessageParam(\n                    tool_call_id=tool_call.id,\n                    role=\"tool\",\n                    content=result_str,\n                )\n            )\n\n    raise RuntimeError(f\"Maximum number of tool calls ({max_tool_calls}) exceeded\")\n</code></pre>"},{"location":"api/client/#openwebui_client.client.OpenWebUIClient.models","title":"<code>models()</code>","text":"<p>Return the custom OpenWebUIModels instance.</p> Source code in <code>openwebui_client/client.py</code> <pre><code>@cached_property\ndef models(self) -&gt; OpenWebUIModels:\n    \"\"\"Return the custom OpenWebUIModels instance.\"\"\"\n    return OpenWebUIModels(self)\n</code></pre>"},{"location":"api/completions/","title":"Chat Completions","text":""},{"location":"api/completions/#openwebui_client.completions","title":"<code>openwebui_client.completions</code>","text":"<p>OpenWebUI completions class for handling file parameters in chat completions.</p>"},{"location":"api/completions/#openwebui_client.completions.OpenWebUICompletions","title":"<code>OpenWebUICompletions</code>","text":"<p>               Bases: <code>Completions</code></p> <p>Extended Completions class that supports the 'files' parameter for OpenWebUI.</p> Source code in <code>openwebui_client/completions.py</code> <pre><code>class OpenWebUICompletions(Completions):\n    \"\"\"Extended Completions class that supports the 'files' parameter for OpenWebUI.\"\"\"\n\n    def __init__(self, client: OpenAI) -&gt; None:\n        \"\"\"Initialize the OpenWebUI completions handler.\n\n        Args:\n            client: The OpenAI client to use for requests\n        \"\"\"\n        # Pass the full OpenAI client, not just its internal client\n        super().__init__(client=client)\n\n    @required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])\n    def create(  # pyright: ignore[reportIncompatibleMethodOverride]\n        self,\n        *,\n        messages: Iterable[ChatCompletionMessageParam],\n        model: Union[str, ChatModel],\n        audio: Union[Optional[ChatCompletionAudioParam], NotGiven] = NOT_GIVEN,\n        files: Union[Optional[Collection[FileObject]], NotGiven] = NOT_GIVEN,\n        frequency_penalty: Union[Optional[float], NotGiven] = NOT_GIVEN,\n        function_call: Union[\n            completion_create_params.FunctionCall, NotGiven\n        ] = NOT_GIVEN,\n        functions: Union[\n            Iterable[completion_create_params.Function], NotGiven\n        ] = NOT_GIVEN,\n        logit_bias: Union[Optional[Dict[str, int]], NotGiven] = NOT_GIVEN,\n        logprobs: Union[Optional[bool], NotGiven] = NOT_GIVEN,\n        max_completion_tokens: Union[Optional[int], NotGiven] = NOT_GIVEN,\n        max_tokens: Union[Optional[int], NotGiven] = NOT_GIVEN,\n        metadata: Union[Optional[Metadata], NotGiven] = NOT_GIVEN,\n        modalities: Union[\n            Optional[List[Literal[\"text\", \"audio\"]]], NotGiven\n        ] = NOT_GIVEN,\n        n: Union[Optional[int], NotGiven] = NOT_GIVEN,\n        parallel_tool_calls: Union[bool, NotGiven] = NOT_GIVEN,\n        prediction: Union[\n            Optional[ChatCompletionPredictionContentParam], NotGiven\n        ] = NOT_GIVEN,\n        presence_penalty: Union[Optional[float], NotGiven] = NOT_GIVEN,\n        reasoning_effort: Union[Optional[ReasoningEffort], NotGiven] = NOT_GIVEN,\n        response_format: Union[\n            completion_create_params.ResponseFormat, NotGiven\n        ] = NOT_GIVEN,\n        seed: Union[Optional[int], NotGiven] = NOT_GIVEN,\n        service_tier: Union[\n            Optional[Literal[\"auto\", \"default\", \"flex\"]], NotGiven\n        ] = NOT_GIVEN,\n        stop: Union[Optional[str], List[str], None, NotGiven] = NOT_GIVEN,\n        store: Union[Optional[bool], NotGiven] = NOT_GIVEN,\n        stream: Union[Optional[Literal[False]], Literal[True], NotGiven] = NOT_GIVEN,\n        stream_options: Union[\n            Optional[ChatCompletionStreamOptionsParam], NotGiven\n        ] = NOT_GIVEN,\n        temperature: Union[Optional[float], NotGiven] = NOT_GIVEN,\n        tool_choice: Union[ChatCompletionToolChoiceOptionParam, NotGiven] = NOT_GIVEN,\n        tools: Union[Iterable[ChatCompletionToolParam], NotGiven] = NOT_GIVEN,\n        top_logprobs: Union[Optional[int], NotGiven] = NOT_GIVEN,\n        top_p: Union[Optional[float], NotGiven] = NOT_GIVEN,\n        user: Union[str, NotGiven] = NOT_GIVEN,\n        web_search_options: Union[\n            completion_create_params.WebSearchOptions, NotGiven\n        ] = NOT_GIVEN,\n        # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n        # The extra values given here take precedence over values defined on the client or passed to this method.\n        extra_headers: Union[Headers, None] = None,\n        extra_query: Union[Query, None] = None,\n        extra_body: Union[Body, None] = None,\n        timeout: Union[float, httpx.Timeout, None, NotGiven] = NOT_GIVEN,\n    ) -&gt; Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n        \"\"\"Create a chat completion with support for the 'files' parameter.\n\n        This overrides the standard create method to handle the 'files' parameter\n        that OpenWebUI supports but is not in the standard OpenAI API.\n\n        Args:\n            messages: A list of messages comprising the conversation so far.\n            model: ID of the model to use.\n            files: A list of file IDs to attach to the completion request (OpenWebUI specific).\n\n            # Standard OpenAI parameters, see OpenAI API docs for details\n            audio: Audio input parameters.\n            frequency_penalty: Penalizes repeated tokens according to frequency.\n            function_call: Controls how the model uses functions.\n            functions: Functions the model may call to interact with external systems.\n            logit_bias: Modifies likelihood of specific tokens appearing in completion.\n            logprobs: Whether to return log probabilities of the output tokens.\n            max_completion_tokens: Maximum number of tokens that can be generated for completions.\n            max_tokens: Maximum number of tokens to generate in the response.\n            metadata: Additional metadata to include in the completion.\n            modalities: List of modalities the model should handle.\n            n: How many completions to generate for each prompt.\n            parallel_tool_calls: Whether function and tool calls should be made in parallel.\n            prediction: Control specifics of prediction content.\n            presence_penalty: Penalizes new tokens based on their presence so far.\n            reasoning_effort: Controls how much effort the model spends reasoning.\n            response_format: Format in which the model should generate responses.\n            seed: Enables deterministic sampling for consistent outputs.\n            service_tier: The service tier to use for the request.\n            stop: Sequences where the API will stop generating further tokens.\n            store: Whether to persist completion for future retrieval.\n            stream: Whether to stream back partial progress.\n            stream_options: Options for streaming responses.\n            temperature: Controls randomness in the response.\n            tool_choice: Controls how the model selects tools.\n            tools: List of tools the model may call.\n            top_logprobs: Number of log probabilities to return per token.\n            top_p: Controls diversity via nucleus sampling.\n            user: Unique identifier representing your end-user.\n            web_search_options: Options to configure web search behavior.\n\n            # Additional parameters for HTTP requests\n            extra_headers: Additional HTTP headers.\n            extra_query: Additional query parameters.\n            extra_body: Additional body parameters.\n            timeout: Request timeout in seconds.\n\n        Returns:\n            A ChatCompletion object containing the model's response.\n        \"\"\"\n        # Extract and handle the 'files' parameter specially\n        # Handle special case for files parameter\n        if files:\n            _logger.debug(f\"Including {len(files)} files in chat completion request\")\n\n            # When files are provided, we need to handle the request manually\n            # because the OpenAI API doesn't support this parameter\n\n            # Create a dictionary of parameters for the API call, excluding special parameters\n            request_data = {\n                k: v\n                for k, v in locals().items()\n                if k != \"self\" and (k is not None or k != NOT_GIVEN) and \"__\" not in k\n            }\n\n            # Make the request using direct HTTP request\n            # OpenWebUI requires files as a parameter in the form data\n\n            import requests\n\n            # Extract the base URL from the client\n            base_url = str(self._client.base_url).rstrip(\"/\")\n\n            # Construct the full URL - try the api prefix again\n            url = f\"{base_url}/chat/completions\"\n\n            # Set up authentication headers and content type for JSON\n            headers = {\n                \"Authorization\": f\"Bearer {self._client.api_key}\",\n                \"Content-Type\": \"application/json\",\n            }\n\n            # Let's try another approach - create a JSON payload with all parameters\n            # Then add that payload as 'json' parameter in form data\n            payload = {\n                \"model\": model,\n                \"messages\": [\n                    {\"role\": m[\"role\"], \"content\": m.get(\"content\")} for m in messages\n                ],\n                \"max_tokens\": max_tokens if max_tokens is not NOT_GIVEN else None,\n            }\n\n            # Add any additional OpenAI parameters to the payload\n            for key, value in request_data.items():\n                if (\n                    key not in [\"self\", \"files\", \"messages\", \"model\", \"max_tokens\"]\n                    and value is not NOT_GIVEN\n                    and value is not None\n                ):\n                    payload[key] = value\n\n            # Add file references if provided\n            # Try sending file IDs in the format OpenWebUI expects\n            if files:\n                # Based on error messages, let's try a different format\n                # Check OpenWebUI's API source to see expected format\n                [f.id for f in files]\n\n                # Format files exactly as shown in OpenWebUI's API docs\n                formatted_files = [{\"type\": \"file\", \"id\": f.id} for f in files]\n                payload[\"files\"] = formatted_files\n\n                # Log additional debug info about the file objects\n                for i, file in enumerate(files):\n                    _logger.debug(\n                        f\"File {i} details: id={file.id}, filename={getattr(file, 'filename', None)}\"\n                    )\n\n            # No need for form data structure, send the payload directly as JSON\n\n            # Print detailed request information\n            _logger.debug(f\"CHAT API - URL: {url}\")\n            _logger.debug(f\"CHAT API - Headers: {headers}\")\n            _logger.debug(f\"CHAT API - Payload: {payload}\")\n            float_timeout: float\n            if timeout is not NOT_GIVEN and timeout is not None:\n                if isinstance(timeout, Timeout):\n                    float_timeout = (timeout.connect or 0) + (timeout.read or 0) or 60\n                    if float_timeout == 0:\n                        float_timeout = 60.0\n                else:\n                    float_timeout = float(timeout or 60.0)\n            else:\n                float_timeout = 60.0\n\n            # Make the HTTP request with JSON payload\n            http_response = requests.post(\n                url, headers=headers, json=payload, timeout=float_timeout\n            )\n\n            # Print response details\n            _logger.debug(f\"CHAT API - Response Status: {http_response.status_code}\")\n            _logger.debug(f\"CHAT API - Response Headers: {dict(http_response.headers)}\")\n            _logger.debug(\n                f\"CHAT API - Response Body: {http_response.text[:500]}...\"\n                if len(http_response.text) &gt; 500\n                else f\"CHAT API - Response Body: {http_response.text}\"\n            )\n\n            # Raise an exception for any HTTP error\n            http_response.raise_for_status()\n\n            # Parse the JSON response\n            response_data = http_response.json()\n\n            # Convert the response to a ChatCompletion object\n            response = ChatCompletion(**response_data)\n            return response\n        else:\n            # Without files, delegate to the parent implementation\n            # Just don't pass the 'files' parameter which is None anyway\n            standard_kwargs = {\n                k: v\n                for k, v in locals().items()\n                if k not in [\"self\", \"files\"] and \"__\" not in k\n            }\n            return super().create(**standard_kwargs)\n</code></pre>"},{"location":"api/completions/#openwebui_client.completions.OpenWebUICompletions.__init__","title":"<code>__init__(client)</code>","text":"<p>Initialize the OpenWebUI completions handler.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>OpenAI</code> <p>The OpenAI client to use for requests</p> required Source code in <code>openwebui_client/completions.py</code> <pre><code>def __init__(self, client: OpenAI) -&gt; None:\n    \"\"\"Initialize the OpenWebUI completions handler.\n\n    Args:\n        client: The OpenAI client to use for requests\n    \"\"\"\n    # Pass the full OpenAI client, not just its internal client\n    super().__init__(client=client)\n</code></pre>"},{"location":"api/completions/#openwebui_client.completions.OpenWebUICompletions.create","title":"<code>create(*, messages, model, audio=NOT_GIVEN, files=NOT_GIVEN, frequency_penalty=NOT_GIVEN, function_call=NOT_GIVEN, functions=NOT_GIVEN, logit_bias=NOT_GIVEN, logprobs=NOT_GIVEN, max_completion_tokens=NOT_GIVEN, max_tokens=NOT_GIVEN, metadata=NOT_GIVEN, modalities=NOT_GIVEN, n=NOT_GIVEN, parallel_tool_calls=NOT_GIVEN, prediction=NOT_GIVEN, presence_penalty=NOT_GIVEN, reasoning_effort=NOT_GIVEN, response_format=NOT_GIVEN, seed=NOT_GIVEN, service_tier=NOT_GIVEN, stop=NOT_GIVEN, store=NOT_GIVEN, stream=NOT_GIVEN, stream_options=NOT_GIVEN, temperature=NOT_GIVEN, tool_choice=NOT_GIVEN, tools=NOT_GIVEN, top_logprobs=NOT_GIVEN, top_p=NOT_GIVEN, user=NOT_GIVEN, web_search_options=NOT_GIVEN, extra_headers=None, extra_query=None, extra_body=None, timeout=NOT_GIVEN)</code>","text":"<p>Create a chat completion with support for the 'files' parameter.</p> <p>This overrides the standard create method to handle the 'files' parameter that OpenWebUI supports but is not in the standard OpenAI API.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Iterable[ChatCompletionMessageParam]</code> <p>A list of messages comprising the conversation so far.</p> required <code>model</code> <code>Union[str, ChatModel]</code> <p>ID of the model to use.</p> required <code>files</code> <code>Union[Optional[Collection[FileObject]], NotGiven]</code> <p>A list of file IDs to attach to the completion request (OpenWebUI specific).</p> <code>NOT_GIVEN</code> <code>audio</code> <code>Union[Optional[ChatCompletionAudioParam], NotGiven]</code> <p>Audio input parameters.</p> <code>NOT_GIVEN</code> <code>frequency_penalty</code> <code>Union[Optional[float], NotGiven]</code> <p>Penalizes repeated tokens according to frequency.</p> <code>NOT_GIVEN</code> <code>function_call</code> <code>Union[FunctionCall, NotGiven]</code> <p>Controls how the model uses functions.</p> <code>NOT_GIVEN</code> <code>functions</code> <code>Union[Iterable[Function], NotGiven]</code> <p>Functions the model may call to interact with external systems.</p> <code>NOT_GIVEN</code> <code>logit_bias</code> <code>Union[Optional[Dict[str, int]], NotGiven]</code> <p>Modifies likelihood of specific tokens appearing in completion.</p> <code>NOT_GIVEN</code> <code>logprobs</code> <code>Union[Optional[bool], NotGiven]</code> <p>Whether to return log probabilities of the output tokens.</p> <code>NOT_GIVEN</code> <code>max_completion_tokens</code> <code>Union[Optional[int], NotGiven]</code> <p>Maximum number of tokens that can be generated for completions.</p> <code>NOT_GIVEN</code> <code>max_tokens</code> <code>Union[Optional[int], NotGiven]</code> <p>Maximum number of tokens to generate in the response.</p> <code>NOT_GIVEN</code> <code>metadata</code> <code>Union[Optional[Metadata], NotGiven]</code> <p>Additional metadata to include in the completion.</p> <code>NOT_GIVEN</code> <code>modalities</code> <code>Union[Optional[List[Literal['text', 'audio']]], NotGiven]</code> <p>List of modalities the model should handle.</p> <code>NOT_GIVEN</code> <code>n</code> <code>Union[Optional[int], NotGiven]</code> <p>How many completions to generate for each prompt.</p> <code>NOT_GIVEN</code> <code>parallel_tool_calls</code> <code>Union[bool, NotGiven]</code> <p>Whether function and tool calls should be made in parallel.</p> <code>NOT_GIVEN</code> <code>prediction</code> <code>Union[Optional[ChatCompletionPredictionContentParam], NotGiven]</code> <p>Control specifics of prediction content.</p> <code>NOT_GIVEN</code> <code>presence_penalty</code> <code>Union[Optional[float], NotGiven]</code> <p>Penalizes new tokens based on their presence so far.</p> <code>NOT_GIVEN</code> <code>reasoning_effort</code> <code>Union[Optional[ReasoningEffort], NotGiven]</code> <p>Controls how much effort the model spends reasoning.</p> <code>NOT_GIVEN</code> <code>response_format</code> <code>Union[ResponseFormat, NotGiven]</code> <p>Format in which the model should generate responses.</p> <code>NOT_GIVEN</code> <code>seed</code> <code>Union[Optional[int], NotGiven]</code> <p>Enables deterministic sampling for consistent outputs.</p> <code>NOT_GIVEN</code> <code>service_tier</code> <code>Union[Optional[Literal['auto', 'default', 'flex']], NotGiven]</code> <p>The service tier to use for the request.</p> <code>NOT_GIVEN</code> <code>stop</code> <code>Union[Optional[str], List[str], None, NotGiven]</code> <p>Sequences where the API will stop generating further tokens.</p> <code>NOT_GIVEN</code> <code>store</code> <code>Union[Optional[bool], NotGiven]</code> <p>Whether to persist completion for future retrieval.</p> <code>NOT_GIVEN</code> <code>stream</code> <code>Union[Optional[Literal[False]], Literal[True], NotGiven]</code> <p>Whether to stream back partial progress.</p> <code>NOT_GIVEN</code> <code>stream_options</code> <code>Union[Optional[ChatCompletionStreamOptionsParam], NotGiven]</code> <p>Options for streaming responses.</p> <code>NOT_GIVEN</code> <code>temperature</code> <code>Union[Optional[float], NotGiven]</code> <p>Controls randomness in the response.</p> <code>NOT_GIVEN</code> <code>tool_choice</code> <code>Union[ChatCompletionToolChoiceOptionParam, NotGiven]</code> <p>Controls how the model selects tools.</p> <code>NOT_GIVEN</code> <code>tools</code> <code>Union[Iterable[ChatCompletionToolParam], NotGiven]</code> <p>List of tools the model may call.</p> <code>NOT_GIVEN</code> <code>top_logprobs</code> <code>Union[Optional[int], NotGiven]</code> <p>Number of log probabilities to return per token.</p> <code>NOT_GIVEN</code> <code>top_p</code> <code>Union[Optional[float], NotGiven]</code> <p>Controls diversity via nucleus sampling.</p> <code>NOT_GIVEN</code> <code>user</code> <code>Union[str, NotGiven]</code> <p>Unique identifier representing your end-user.</p> <code>NOT_GIVEN</code> <code>web_search_options</code> <code>Union[WebSearchOptions, NotGiven]</code> <p>Options to configure web search behavior.</p> <code>NOT_GIVEN</code> <code>extra_headers</code> <code>Union[Headers, None]</code> <p>Additional HTTP headers.</p> <code>None</code> <code>extra_query</code> <code>Union[Query, None]</code> <p>Additional query parameters.</p> <code>None</code> <code>extra_body</code> <code>Union[Body, None]</code> <p>Additional body parameters.</p> <code>None</code> <code>timeout</code> <code>Union[float, Timeout, None, NotGiven]</code> <p>Request timeout in seconds.</p> <code>NOT_GIVEN</code> <p>Returns:</p> Type Description <code>Union[ChatCompletion, Stream[ChatCompletionChunk]]</code> <p>A ChatCompletion object containing the model's response.</p> Source code in <code>openwebui_client/completions.py</code> <pre><code>@required_args([\"messages\", \"model\"], [\"messages\", \"model\", \"stream\"])\ndef create(  # pyright: ignore[reportIncompatibleMethodOverride]\n    self,\n    *,\n    messages: Iterable[ChatCompletionMessageParam],\n    model: Union[str, ChatModel],\n    audio: Union[Optional[ChatCompletionAudioParam], NotGiven] = NOT_GIVEN,\n    files: Union[Optional[Collection[FileObject]], NotGiven] = NOT_GIVEN,\n    frequency_penalty: Union[Optional[float], NotGiven] = NOT_GIVEN,\n    function_call: Union[\n        completion_create_params.FunctionCall, NotGiven\n    ] = NOT_GIVEN,\n    functions: Union[\n        Iterable[completion_create_params.Function], NotGiven\n    ] = NOT_GIVEN,\n    logit_bias: Union[Optional[Dict[str, int]], NotGiven] = NOT_GIVEN,\n    logprobs: Union[Optional[bool], NotGiven] = NOT_GIVEN,\n    max_completion_tokens: Union[Optional[int], NotGiven] = NOT_GIVEN,\n    max_tokens: Union[Optional[int], NotGiven] = NOT_GIVEN,\n    metadata: Union[Optional[Metadata], NotGiven] = NOT_GIVEN,\n    modalities: Union[\n        Optional[List[Literal[\"text\", \"audio\"]]], NotGiven\n    ] = NOT_GIVEN,\n    n: Union[Optional[int], NotGiven] = NOT_GIVEN,\n    parallel_tool_calls: Union[bool, NotGiven] = NOT_GIVEN,\n    prediction: Union[\n        Optional[ChatCompletionPredictionContentParam], NotGiven\n    ] = NOT_GIVEN,\n    presence_penalty: Union[Optional[float], NotGiven] = NOT_GIVEN,\n    reasoning_effort: Union[Optional[ReasoningEffort], NotGiven] = NOT_GIVEN,\n    response_format: Union[\n        completion_create_params.ResponseFormat, NotGiven\n    ] = NOT_GIVEN,\n    seed: Union[Optional[int], NotGiven] = NOT_GIVEN,\n    service_tier: Union[\n        Optional[Literal[\"auto\", \"default\", \"flex\"]], NotGiven\n    ] = NOT_GIVEN,\n    stop: Union[Optional[str], List[str], None, NotGiven] = NOT_GIVEN,\n    store: Union[Optional[bool], NotGiven] = NOT_GIVEN,\n    stream: Union[Optional[Literal[False]], Literal[True], NotGiven] = NOT_GIVEN,\n    stream_options: Union[\n        Optional[ChatCompletionStreamOptionsParam], NotGiven\n    ] = NOT_GIVEN,\n    temperature: Union[Optional[float], NotGiven] = NOT_GIVEN,\n    tool_choice: Union[ChatCompletionToolChoiceOptionParam, NotGiven] = NOT_GIVEN,\n    tools: Union[Iterable[ChatCompletionToolParam], NotGiven] = NOT_GIVEN,\n    top_logprobs: Union[Optional[int], NotGiven] = NOT_GIVEN,\n    top_p: Union[Optional[float], NotGiven] = NOT_GIVEN,\n    user: Union[str, NotGiven] = NOT_GIVEN,\n    web_search_options: Union[\n        completion_create_params.WebSearchOptions, NotGiven\n    ] = NOT_GIVEN,\n    # Use the following arguments if you need to pass additional parameters to the API that aren't available via kwargs.\n    # The extra values given here take precedence over values defined on the client or passed to this method.\n    extra_headers: Union[Headers, None] = None,\n    extra_query: Union[Query, None] = None,\n    extra_body: Union[Body, None] = None,\n    timeout: Union[float, httpx.Timeout, None, NotGiven] = NOT_GIVEN,\n) -&gt; Union[ChatCompletion, Stream[ChatCompletionChunk]]:\n    \"\"\"Create a chat completion with support for the 'files' parameter.\n\n    This overrides the standard create method to handle the 'files' parameter\n    that OpenWebUI supports but is not in the standard OpenAI API.\n\n    Args:\n        messages: A list of messages comprising the conversation so far.\n        model: ID of the model to use.\n        files: A list of file IDs to attach to the completion request (OpenWebUI specific).\n\n        # Standard OpenAI parameters, see OpenAI API docs for details\n        audio: Audio input parameters.\n        frequency_penalty: Penalizes repeated tokens according to frequency.\n        function_call: Controls how the model uses functions.\n        functions: Functions the model may call to interact with external systems.\n        logit_bias: Modifies likelihood of specific tokens appearing in completion.\n        logprobs: Whether to return log probabilities of the output tokens.\n        max_completion_tokens: Maximum number of tokens that can be generated for completions.\n        max_tokens: Maximum number of tokens to generate in the response.\n        metadata: Additional metadata to include in the completion.\n        modalities: List of modalities the model should handle.\n        n: How many completions to generate for each prompt.\n        parallel_tool_calls: Whether function and tool calls should be made in parallel.\n        prediction: Control specifics of prediction content.\n        presence_penalty: Penalizes new tokens based on their presence so far.\n        reasoning_effort: Controls how much effort the model spends reasoning.\n        response_format: Format in which the model should generate responses.\n        seed: Enables deterministic sampling for consistent outputs.\n        service_tier: The service tier to use for the request.\n        stop: Sequences where the API will stop generating further tokens.\n        store: Whether to persist completion for future retrieval.\n        stream: Whether to stream back partial progress.\n        stream_options: Options for streaming responses.\n        temperature: Controls randomness in the response.\n        tool_choice: Controls how the model selects tools.\n        tools: List of tools the model may call.\n        top_logprobs: Number of log probabilities to return per token.\n        top_p: Controls diversity via nucleus sampling.\n        user: Unique identifier representing your end-user.\n        web_search_options: Options to configure web search behavior.\n\n        # Additional parameters for HTTP requests\n        extra_headers: Additional HTTP headers.\n        extra_query: Additional query parameters.\n        extra_body: Additional body parameters.\n        timeout: Request timeout in seconds.\n\n    Returns:\n        A ChatCompletion object containing the model's response.\n    \"\"\"\n    # Extract and handle the 'files' parameter specially\n    # Handle special case for files parameter\n    if files:\n        _logger.debug(f\"Including {len(files)} files in chat completion request\")\n\n        # When files are provided, we need to handle the request manually\n        # because the OpenAI API doesn't support this parameter\n\n        # Create a dictionary of parameters for the API call, excluding special parameters\n        request_data = {\n            k: v\n            for k, v in locals().items()\n            if k != \"self\" and (k is not None or k != NOT_GIVEN) and \"__\" not in k\n        }\n\n        # Make the request using direct HTTP request\n        # OpenWebUI requires files as a parameter in the form data\n\n        import requests\n\n        # Extract the base URL from the client\n        base_url = str(self._client.base_url).rstrip(\"/\")\n\n        # Construct the full URL - try the api prefix again\n        url = f\"{base_url}/chat/completions\"\n\n        # Set up authentication headers and content type for JSON\n        headers = {\n            \"Authorization\": f\"Bearer {self._client.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n        # Let's try another approach - create a JSON payload with all parameters\n        # Then add that payload as 'json' parameter in form data\n        payload = {\n            \"model\": model,\n            \"messages\": [\n                {\"role\": m[\"role\"], \"content\": m.get(\"content\")} for m in messages\n            ],\n            \"max_tokens\": max_tokens if max_tokens is not NOT_GIVEN else None,\n        }\n\n        # Add any additional OpenAI parameters to the payload\n        for key, value in request_data.items():\n            if (\n                key not in [\"self\", \"files\", \"messages\", \"model\", \"max_tokens\"]\n                and value is not NOT_GIVEN\n                and value is not None\n            ):\n                payload[key] = value\n\n        # Add file references if provided\n        # Try sending file IDs in the format OpenWebUI expects\n        if files:\n            # Based on error messages, let's try a different format\n            # Check OpenWebUI's API source to see expected format\n            [f.id for f in files]\n\n            # Format files exactly as shown in OpenWebUI's API docs\n            formatted_files = [{\"type\": \"file\", \"id\": f.id} for f in files]\n            payload[\"files\"] = formatted_files\n\n            # Log additional debug info about the file objects\n            for i, file in enumerate(files):\n                _logger.debug(\n                    f\"File {i} details: id={file.id}, filename={getattr(file, 'filename', None)}\"\n                )\n\n        # No need for form data structure, send the payload directly as JSON\n\n        # Print detailed request information\n        _logger.debug(f\"CHAT API - URL: {url}\")\n        _logger.debug(f\"CHAT API - Headers: {headers}\")\n        _logger.debug(f\"CHAT API - Payload: {payload}\")\n        float_timeout: float\n        if timeout is not NOT_GIVEN and timeout is not None:\n            if isinstance(timeout, Timeout):\n                float_timeout = (timeout.connect or 0) + (timeout.read or 0) or 60\n                if float_timeout == 0:\n                    float_timeout = 60.0\n            else:\n                float_timeout = float(timeout or 60.0)\n        else:\n            float_timeout = 60.0\n\n        # Make the HTTP request with JSON payload\n        http_response = requests.post(\n            url, headers=headers, json=payload, timeout=float_timeout\n        )\n\n        # Print response details\n        _logger.debug(f\"CHAT API - Response Status: {http_response.status_code}\")\n        _logger.debug(f\"CHAT API - Response Headers: {dict(http_response.headers)}\")\n        _logger.debug(\n            f\"CHAT API - Response Body: {http_response.text[:500]}...\"\n            if len(http_response.text) &gt; 500\n            else f\"CHAT API - Response Body: {http_response.text}\"\n        )\n\n        # Raise an exception for any HTTP error\n        http_response.raise_for_status()\n\n        # Parse the JSON response\n        response_data = http_response.json()\n\n        # Convert the response to a ChatCompletion object\n        response = ChatCompletion(**response_data)\n        return response\n    else:\n        # Without files, delegate to the parent implementation\n        # Just don't pass the 'files' parameter which is None anyway\n        standard_kwargs = {\n            k: v\n            for k, v in locals().items()\n            if k not in [\"self\", \"files\"] and \"__\" not in k\n        }\n        return super().create(**standard_kwargs)\n</code></pre>"},{"location":"api/files/","title":"Files","text":""},{"location":"api/files/#openwebui_client.files","title":"<code>openwebui_client.files</code>","text":"<p>OpenWebUI files class for handling file uploads.</p>"},{"location":"api/files/#openwebui_client.files.OpenWebUIFiles","title":"<code>OpenWebUIFiles</code>","text":"<p>               Bases: <code>Files</code></p> <p>Extended Files class for OpenWebUI with improved file upload functionality.</p> Source code in <code>openwebui_client/files.py</code> <pre><code>class OpenWebUIFiles(Files):\n    \"\"\"Extended Files class for OpenWebUI with improved file upload functionality.\"\"\"\n\n    def from_paths(\n        self,\n        files: Iterable[Tuple[Path, Optional[Dict[str, Any]]]],\n    ) -&gt; List[FileObject]:\n        return [self.from_path(file, file_metadata) for file, file_metadata in files]\n\n    def from_path(\n        self,\n        file: Path,\n        file_metadata: Optional[Dict[str, Any]] = None,\n    ) -&gt; FileObject:\n        with file.open(\"rb\") as filestream:\n            # OpenWebUI requires a specific format for file uploads\n            # The key differences from standard OpenAI:\n            # 1. Using a trailing slash on the endpoint path\n            # 2. Adding a 'process=true' parameter\n            # 3. Using the proper multipart/form-data format for the file\n\n            # Use direct HTTP request instead of the OpenAI client for file uploads\n            import requests\n\n            # Extract the base URL from the client (removing any trailing slash)\n            base_url = str(self._client.base_url).rstrip(\"/\")\n\n            # Construct the full URL with the required trailing slash\n            url = f\"{base_url}/v1/files/\"\n\n            # Set up authentication headers\n            headers = {\"Authorization\": f\"Bearer {self._client.api_key}\"}\n\n            # Set up the multipart form data like the curl command\n            # Place both the file and process=true in the files parameter\n            # This matches how curl -F works\n            files = {\n                \"file\": filestream,\n            }\n            data = {\"process\": \"true\"}\n\n            # Add any additional metadata provided by the user\n            if file_metadata:\n                for key, value in file_metadata.items():\n                    data[key] = str(value)\n\n            # Print detailed request information\n            _logger.debug(f\"FILES API - URL: {url}\")\n            _logger.debug(f\"FILES API - Headers: {headers}\")\n            _logger.debug(f\"FILES API - Data: {data}\")\n            _logger.debug(f\"FILES API - Files: {files.keys()}\")\n\n            # Make the HTTP request directly\n            http_response = requests.post(\n                url, headers=headers, files=files, data=data, timeout=60\n            )\n\n            # Print response details\n            _logger.debug(f\"FILES API - Response Status: {http_response.status_code}\")\n            _logger.debug(\n                f\"FILES API - Response Headers: {dict(http_response.headers)}\"\n            )\n            _logger.debug(\n                f\"FILES API - Response Body: {http_response.text[:500]}...\"\n                if len(http_response.text) &gt; 500\n                else f\"FILES API - Response Body: {http_response.text}\"\n            )\n\n            # Raise an exception for any HTTP error\n            http_response.raise_for_status()\n\n            # Parse the JSON response\n            response_data = http_response.json()\n\n            if response_data.get(\"error\"):\n                raise ValueError(response_data.get(\"error\"))\n\n            # Convert the response to an OpenAI FileObject with required defaults\n            file_object = FileObject(\n                id=response_data.get(\"id\", f\"file-{str(file.name)}\"),\n                bytes=response_data.get(\n                    \"bytes\", file.stat().st_size\n                ),  # Default to file size\n                created_at=response_data.get(\n                    \"created_at\", int(time.time())\n                ),  # Default to current time\n                filename=response_data.get(\"filename\", file.name),\n                object=\"file\",  # Required fixed value\n                purpose=response_data.get(\"purpose\", \"assistants\"),  # Default purpose\n                status=response_data.get(\"status\", \"processed\"),  # Default status\n                status_details=response_data.get(\"status_details\"),\n            )\n\n            return file_object\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>The OpenWebUI client provides access to the models available in your OpenWebUI instance through the <code>models</code> property of the client.</p>"},{"location":"api/models/#listing-models","title":"Listing Models","text":"<p>You can list all available models using the <code>list()</code> method:</p> <pre><code>from openwebui_client import OpenWebUIClient\n\nclient = OpenWebUIClient(\n    api_key=\"your-api-key\",\n    base_url=\"https://your-openwebui-instance.com\"\n)\n\n# List all available models\nmodels = client.models.list()\n\n# Print model information\nfor model in models:\n    print(f\"ID: {model.id}, Name: {model.name}, Owner: {model.owned_by}\")\n</code></pre>"},{"location":"api/models/#model-object","title":"Model Object","text":"<p>The <code>OpenWebUIModel</code> object has the following properties:</p> <ul> <li><code>id</code> (str): The unique identifier for the model</li> <li><code>name</code> (Optional[str]): Human-readable name of the model (may be None)</li> <li><code>created</code> (int): Unix timestamp for when the model was created</li> <li><code>object</code> (str): Always \"model\"</li> <li><code>owned_by</code> (str): Organization that owns the model</li> </ul>"},{"location":"api/models/#implementation-notes","title":"Implementation Notes","text":"<p>The OpenWebUI client extends the OpenAI client's Models class to handle the different response format from OpenWebUI's API. The models endpoint in OpenWebUI is located at <code>/models</code> rather than <code>/v1/models</code> as in the OpenAI API.</p> <p>The client automatically adds the appropriate headers to ensure JSON responses from the API.</p>"}]}